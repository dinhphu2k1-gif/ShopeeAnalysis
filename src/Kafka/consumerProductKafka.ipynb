{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import requests\n",
    "from json import dumps\n",
    "import time\n",
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'Product',\n",
    "     bootstrap_servers=['127.0.0.1:9092'],\n",
    "     auto_offset_reset='earliest',\n",
    "     enable_auto_commit=True,\n",
    "     group_id='my-group',\n",
    "     value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"Upload Product to HDFS\").setMaster(\"spark://25.19.134.189:7077\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Upload Product to HDFS').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Upload Product to HDFS\n",
      "999\n",
      "1999\n",
      "2999\n",
      "3999\n",
      "4999\n",
      "Upload Product to HDFS\n",
      "998\n",
      "1998\n",
      "2998\n",
      "3998\n",
      "4998\n",
      "Upload Product to HDFS\n",
      "997\n",
      "1997\n",
      "2997\n",
      "3997\n",
      "4997\n",
      "Upload Product to HDFS\n",
      "996\n",
      "1996\n",
      "2996\n",
      "3996\n",
      "4996\n",
      "Upload Product to HDFS\n",
      "995\n",
      "1995\n",
      "2995\n",
      "3995\n",
      "4995\n",
      "Upload Product to HDFS\n",
      "994\n",
      "1994\n",
      "2994\n",
      "3994\n",
      "4994\n",
      "Upload Product to HDFS\n",
      "993\n",
      "1993\n",
      "2993\n",
      "3993\n",
      "4993\n",
      "Upload Product to HDFS\n",
      "992\n",
      "1992\n",
      "2992\n",
      "3992\n",
      "4992\n",
      "Upload Product to HDFS\n",
      "991\n",
      "1991\n",
      "2991\n",
      "3991\n",
      "4991\n",
      "Upload Product to HDFS\n",
      "990\n",
      "1990\n",
      "2990\n",
      "3990\n",
      "4990\n",
      "Upload Product to HDFS\n",
      "989\n",
      "1989\n",
      "2989\n",
      "3989\n",
      "4989\n",
      "Upload Product to HDFS\n",
      "988\n",
      "1988\n",
      "2988\n",
      "3988\n",
      "4988\n",
      "Upload Product to HDFS\n",
      "987\n",
      "1987\n",
      "2987\n",
      "3987\n",
      "4987\n",
      "Upload Product to HDFS\n",
      "986\n",
      "1986\n",
      "2986\n",
      "3986\n",
      "4986\n",
      "Upload Product to HDFS\n",
      "985\n",
      "1985\n",
      "2985\n",
      "3985\n",
      "4985\n",
      "Upload Product to HDFS\n"
     ]
    }
   ],
   "source": [
    "df = []\n",
    "iter = 0\n",
    "for message in consumer:\n",
    "    iter +=1\n",
    "    message = message.value\n",
    "    df.append(message)\n",
    "    if iter %1000 ==0:\n",
    "        iter = 0\n",
    "        print(len(df))\n",
    "    if len(df) > 5000:\n",
    "        print('Upload Product to HDFS')\n",
    "        try:\n",
    "            pandas_df = pd.DataFrame(df)\n",
    "            df = []\n",
    "            data = spark.createDataFrame(pandas_df.astype(str))\n",
    "            data.coalesce(1).write.mode('append').csv('hdfs://cris:9000/shopee/Product/')\n",
    "        except:\n",
    "            print('Error when upload data to HDFS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.DataFrame(df)\n",
    "data = spark.createDataFrame(pandas_df.astype(str))\n",
    "data.coalesce(1).write.mode('append').csv('hdfs://cris:9000/shopee/Item/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = spark.read.csv('hdfs://cris:9000/shopee/Item/part-00000-ad86ee4c-be73-4872-9f2f-88c1a8cae58f-c000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0c47fc68ffbbf60d61484e788c5fda40d927438cafb86813efe033e797683a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
